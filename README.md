
**Research Engineer**

**Meta Superinteligence Labs - Fundamental AI Research (MSL - FAIR)**

I work on advancing pre-training and inference-time compute of autoregressive next-token prediction, diffusion, and flow matching models at scale.
In 2024 I obtained a Ph.D. from the University of Toronto and the Vector Institute for A.I. in generative modelling and information theory.
I spent most of grad school interning at Meta (FAIR Labs) and Google AI.
Before grad school I worked as an electronics engineer (hardware/firmware for embedded systems), as well as a machine learning engineer in recommendation systems and ML for health.

[![Google Scholar](https://img.shields.io/badge/Google%20Scholar-4285F4?style=for-the-badge&logo=google-scholar&logoColor=white)
](https://scholar.google.com/citations?user=5bQjLz4AAAAJ&hl=en)
[![X (Twitter)](https://img.shields.io/badge/X-000000.svg?style=for-the-badge&logo=X&logoColor=white)
](https://twitter.com/_dsevero)
[![CV](https://img.shields.io/badge/CV%20&#40;last%20updated%20October%202025&#41;-009900?style=for-the-badge&logoColor=white)](https://drive.google.com/file/d/1vpLWEKpd_YyPJPj78dGnIOYNQrukO92C/view?usp=drivesdk)

## Selected projects from 2024/2025
- Advancing flow matching pre-training (https://arxiv.org/abs/2412.03487)
- Reducing the number of forward passes for code generation with masked diffusion models, without re-training (https://arxiv.org/abs/2505.24857)
- Improving memory efficiency of vector databases (FAISS) via lossless compression, with no impact to search quality (https://arxiv.org/abs/2501.10479)
- Efficient learning and inference of distributions over permutations applied to re-ranking in recommendation systems (https://arxiv.org/abs/2505.24664)

For a complete list, please see my [Google Scholar](https://scholar.google.com/citations?user=5bQjLz4AAAAJ&hl=en) profile.

## More about me
Originally, I am from Florianópolis (Brazil) but I've lived in NYC, Orlando, Toronto, São Paulo, and (now) Montréal, as well as other smaller cities in the south of Brazil.

I obtained a Ph.D. from the **University of Toronto** and the **Vector Institute for A.I.** in Information Theory and Generative Modelling.
My thesis studies, and proposes algorithms for, lossless compression of combinatorial objects such as graphs, multisets, and partitions.
Thesis: [Random Permutation Codes: Lossless Source Coding of Non-Sequential Data](https://arxiv.org/abs/2411.14879)

# Tutorials, Workshops, and Talks in data compression and other things
- [Slides from DCC2025 (click here)](https://nbviewer.org/github/dsevero/dsevero/blob/master/static/DCC2025%20-%20PCircuits.pdf) / [(mirror)](static/DCC2025%20-%20PCircuits.pdf), 2025
- [ICML 2023 Workshop on Neural Compression and Information Theory](https://neuralcompression.github.io/workshop23), 2023
- [Asymmetric Numeral Systems (ANS) codec in pure Python](https://gist.github.com/dsevero/7e02d96e079ce44b89ff33d7a1ce1738), 2021
- [A tutorial on bits-back with Huffman coding](https://gist.github.com/dsevero/8e7c38b44953964d3b9873b6bd96d9b2), 2021
- [Vectorized Run-Length Encoding](https://gist.github.com/dsevero/693677754798e21f539e4e11a3103576), 2021
- [Persisting lru_cache to disk while using hashable pandas objects for parallel experiments](https://gist.github.com/dsevero/252a5f280600c6b1118ed42826d188a9), 2020
